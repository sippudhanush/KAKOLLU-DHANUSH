{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b491076a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving training data and testing data filepath to variables for easier access\n",
    "file_path = r\"https://raw.githubusercontent.com/amankharwal/Website-data/master/stress.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5103a5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the test data and training data and storing them in DataFrame titled as test_data and training_data respectively\n",
    "import pandas as pd\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6be82bb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['subreddit', 'post_id', 'sentence_range', 'text', 'id', 'label',\n",
       "       'confidence', 'social_timestamp', 'social_karma', 'syntax_ari',\n",
       "       ...\n",
       "       'lex_dal_min_pleasantness', 'lex_dal_min_activation',\n",
       "       'lex_dal_min_imagery', 'lex_dal_avg_activation', 'lex_dal_avg_imagery',\n",
       "       'lex_dal_avg_pleasantness', 'social_upvote_ratio',\n",
       "       'social_num_comments', 'syntax_fk_grade', 'sentiment'],\n",
       "      dtype='object', length=116)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f68323d",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_columns = ['text','label']\n",
    "# label -> 1 (stressed) and lebel -> 0(not stressed)\n",
    "data = df[relevant_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7502e362",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text     0\n",
       "label    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a725ab89",
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = data.text\n",
    "labels = data.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e873f48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       1\n",
       "1       0\n",
       "2       1\n",
       "3       1\n",
       "4       1\n",
       "       ..\n",
       "2833    0\n",
       "2834    1\n",
       "2835    0\n",
       "2836    0\n",
       "2837    1\n",
       "Name: label, Length: 2838, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39fefda2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       He said he had not felt that way before, sugge...\n",
       "1       Hey there r/assistance, Not sure if this is th...\n",
       "2       My mom then hit me with the newspaper and it s...\n",
       "3       until i met my new boyfriend, he is amazing, h...\n",
       "4       October is Domestic Violence Awareness Month a...\n",
       "                              ...                        \n",
       "2833    * Her, a week ago: Precious, how are you? (I i...\n",
       "2834    I don't have the ability to cope with it anymo...\n",
       "2835    In case this is the first time you're reading ...\n",
       "2836    Do you find this normal? They have a good rela...\n",
       "2837    I was talking to my mom this morning and she s...\n",
       "Name: text, Length: 2838, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e61b99d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a lemmatizer to convert a word into base form \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a075a2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to convert tags given by pos_tag function to the tags accepted by WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "def get_simple_pos(tag):\n",
    "  if tag.startswith('J'):\n",
    "    return wordnet.ADJ\n",
    "  elif tag.startswith('V'):\n",
    "    return wordnet.VERB\n",
    "  elif tag.startswith('N'):\n",
    "    return wordnet.NOUN\n",
    "  elif tag.startswith('R'):\n",
    "    return wordnet.ADV\n",
    "  else:\n",
    "    return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a2a5703",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# Download the required NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')  # Required for POS tagging\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "# Define the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Define the set of stopwords and punctuations\n",
    "stops = set(stopwords.words('english'))\n",
    "punctuations = list(string.punctuation)\n",
    "stops.update(punctuations)\n",
    "\n",
    "# Define a function to map POS tags to lemmatizer format\n",
    "def get_simple_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return 'a'  # Adjective\n",
    "    elif tag.startswith('V'):\n",
    "        return 'v'  # Verb\n",
    "    elif tag.startswith('N'):\n",
    "        return 'n'  # Noun\n",
    "    elif tag.startswith('R'):\n",
    "        return 'r'  # Adverb\n",
    "    else:\n",
    "        return 'n'  # Default to noun\n",
    "\n",
    "# Define the function to clean the data\n",
    "def clean_data(words):\n",
    "    output_words = []\n",
    "    for w in words:\n",
    "        if w.lower() not in stops:\n",
    "            pos = pos_tag([w])\n",
    "            clean_word = lemmatizer.lemmatize(w, pos=get_simple_pos(pos[0][1]))\n",
    "            output_words.append(clean_word.lower())\n",
    "    return output_words\n",
    "\n",
    "# Example usage of the function\n",
    "example_words = [\"This\", \"is\", \"a\", \"test\", \"sentence\", \".\"]\n",
    "cleaned_words = clean_data(example_words)\n",
    "print(cleaned_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bafba464",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "tokenized_posts = [word_tokenize(post.lower()) for post in posts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "01f12eef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Cleaning the data from each post:\n",
    "clean_posts = [clean_data(post) for post in tokenized_posts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9e724ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = [\" \".join(post) for post in clean_posts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8edf9dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(posts, labels,test_size=0.25, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2b0f6a68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from sklearn.feature_extraction.text import CountVectorizer\\ncount_vec = CountVectorizer(max_features = 3000, ngram_range = (1,2),max_df = 0.8)\\nX_train_features = count_vec.fit_transform(X_train)\\nX_train_features.todense()'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vec = CountVectorizer(max_features = 3000, ngram_range = (1,2),max_df = 0.8)\n",
    "X_train_features = count_vec.fit_transform(X_train)\n",
    "X_train_features.todense()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c620b26f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        ...,\n",
       "        [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.        , 0.        , 0.21240241, ..., 0.        , 0.        ,\n",
       "         0.        ]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "count_vec = TfidfVectorizer(max_features = 3000, ngram_range = (1,3), max_df = 0.8)\n",
    "X_train_features = count_vec.fit_transform(X_train)\n",
    "X_train_features.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e1d6a222",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['00', '000', '10', ..., 'zach', 'zero', 'zoloft'], dtype=object)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To get the feature names selected\n",
    "#count_vec.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2c815379",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<710x3000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 22872 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_features = count_vec.transform(X_test)\n",
    "X_test_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "852141c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.73\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.74      0.71       319\n",
      "           1       0.77      0.72      0.75       391\n",
      "\n",
      "    accuracy                           0.73       710\n",
      "   macro avg       0.73      0.73      0.73       710\n",
      "weighted avg       0.73      0.73      0.73       710\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "svc = SVC()\n",
    "svc.fit(X_train_features, Y_train)\n",
    "svc.score(X_test_features, Y_test)\n",
    "\n",
    "Y_pred = svc.predict(X_test_features)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(Y_test, Y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Print other metrics\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c6e29d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.73\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.62      0.67       319\n",
      "           1       0.72      0.82      0.77       391\n",
      "\n",
      "    accuracy                           0.73       710\n",
      "   macro avg       0.73      0.72      0.72       710\n",
      "weighted avg       0.73      0.73      0.72       710\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Create a Multinomial Naive Bayes classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "\n",
    "# Train the classifier on the vectorized training data\n",
    "nb_classifier.fit(X_train_features, Y_train)\n",
    "\n",
    "# Make predictions on the vectorized test data\n",
    "Y_pred = nb_classifier.predict(X_test_features)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(Y_test, Y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Print other metrics\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(Y_test, Y_pred))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f3d47547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter text to predict its class: i am feeling great today\n",
      "Not Stressed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Take input text from the user\n",
    "user_input = input(\"Enter text to predict its class: \")\n",
    "\n",
    "# Tokenization of the user input\n",
    "tokenized_user_input = word_tokenize(user_input.lower())\n",
    "\n",
    "# stopword removal, lemmatization of tokenized_user_input\n",
    "clean_user_input = clean_data(tokenized_user_input)\n",
    "user_input = \" \".join(clean_user_input)\n",
    "\n",
    "# Vectorize the user input using the pre-trained vectorizer\n",
    "user_input_vectorized = count_vec.transform([user_input])\n",
    "\n",
    "# Make predictions using the pre-trained classifier\n",
    "predicted_class = nb_classifier.predict(user_input_vectorized)\n",
    "\n",
    "if(predicted_class[0] == 0):\n",
    "    print(\"Not Stressed\")\n",
    "elif(predicted_class[0] == 1):\n",
    "   print(\"Stressed\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
